{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "885e73f1-5875-4946-8b61-d226f1cf1802",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, numpy as np, pandas as pd, random, joblib\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "import torch, torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "SEED = 2025\n",
    "random.seed(SEED); np.random.seed(SEED); torch.manual_seed(SEED)\n",
    "BASE = r\"C:\\Users\\81005\\Desktop\\CYH\\3-PFAS\"\n",
    "PATH_TRAIN = os.path.join(BASE, r\"深度学习验证\\merged_all.xlsx\")\n",
    "PATH_NEW   = os.path.join(BASE, r\"大样本预测\\7565_filled.xlsx\")  \n",
    "OUT_DIR    = os.path.join(BASE, r\"大样本预测\")\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "df_tr = pd.read_excel(PATH_TRAIN)\n",
    "assert \"Level_OA\" in df_tr.columns\n",
    "df_tr[\"Level_OA\"] = df_tr[\"Level_OA\"].astype(int)\n",
    "df_new = pd.read_excel(PATH_NEW)\n",
    "\n",
    "EXCLUDE = {\"Name\",\"Level_OA\",\"fold_id\",\"G\",\"P_PI3K\",\"P_PPAR\",\"P_ROS\",\"P_LPS\",\"P_OA\"}\n",
    "num_cols_tr  = [c for c in df_tr.columns  if (c not in EXCLUDE and np.issubdtype(df_tr[c].dtype, np.number))]\n",
    "num_cols_new = [c for c in df_new.columns if  np.issubdtype(df_new[c].dtype, np.number)]\n",
    "COLS_FINAL   = [c for c in num_cols_tr if c in num_cols_new]\n",
    "assert len(COLS_FINAL)>0, \"训练与新数据无共同结构列\"\n",
    "\n",
    "X_all = df_tr[COLS_FINAL].replace([np.inf,-np.inf], np.nan).values\n",
    "y_all = df_tr[\"Level_OA\"].values\n",
    "X_new = df_new[COLS_FINAL].replace([np.inf,-np.inf], np.nan).values\n",
    "\n",
    "imputer = SimpleImputer(strategy=\"median\").fit(X_all)\n",
    "X_all_imp = imputer.transform(X_all)\n",
    "scaler  = StandardScaler().fit(X_all_imp)\n",
    "X_all_std = scaler.transform(X_all_imp)\n",
    "X_new_std = scaler.transform(imputer.transform(X_new))\n",
    "\n",
    "class TinyTabTransformer(nn.Module):\n",
    "    def __init__(self, in_dim, n_classes=3, d_model=64, n_heads=4, n_layers=2, dropout=0.35):\n",
    "        super().__init__()\n",
    "        self.proj = nn.Linear(in_dim, d_model)\n",
    "        enc = nn.TransformerEncoderLayer(d_model=d_model, nhead=n_heads,\n",
    "                                         dim_feedforward=d_model*2, dropout=dropout,\n",
    "                                         batch_first=True, activation=\"gelu\")\n",
    "        self.encoder = nn.TransformerEncoder(enc, num_layers=n_layers)\n",
    "        self.norm = nn.LayerNorm(d_model)\n",
    "        self.head = nn.Sequential(nn.Linear(d_model, d_model//2),\n",
    "                                  nn.GELU(), nn.Dropout(dropout),\n",
    "                                  nn.Linear(d_model//2, n_classes))\n",
    "    def forward(self, x):\n",
    "        x = self.proj(x).unsqueeze(1)    \n",
    "        x = self.encoder(x)\n",
    "        x = self.norm(x.squeeze(1))\n",
    "        return self.head(x)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "n_classes = len(np.unique(y_all))\n",
    "y_all0 = (y_all - y_all.min())  \n",
    "\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "sss = StratifiedShuffleSplit(n_splits=1, test_size=0.1, random_state=SEED)\n",
    "tr_idx, val_idx = next(sss.split(X_all_std, y_all0))\n",
    "X_tr, X_val = X_all_std[tr_idx], X_all_std[val_idx]\n",
    "y_tr, y_val = y_all0[tr_idx], y_all0[val_idx]\n",
    "\n",
    "train_ds = TensorDataset(torch.tensor(X_tr, dtype=torch.float32),\n",
    "                         torch.tensor(y_tr, dtype=torch.long))\n",
    "val_ds   = TensorDataset(torch.tensor(X_val, dtype=torch.float32),\n",
    "                         torch.tensor(y_val, dtype=torch.long))\n",
    "train_loader = DataLoader(train_ds, batch_size=16, shuffle=True)\n",
    "val_loader   = DataLoader(val_ds, batch_size=64, shuffle=False)\n",
    "\n",
    "model = TinyTabTransformer(in_dim=X_all_std.shape[1], n_classes=n_classes,\n",
    "                           d_model=64, n_heads=4, n_layers=2, dropout=0.35).to(device)\n",
    "\n",
    "cls, cnt = np.unique(y_tr, return_counts=True)\n",
    "w = torch.tensor([(len(y_tr)/c) for c in cnt], dtype=torch.float32).to(device)\n",
    "criterion = nn.CrossEntropyLoss(weight=w)\n",
    "optim = torch.optim.Adam(model.parameters(), lr=1e-3, weight_decay=1e-4)\n",
    "\n",
    "best_f1, best_state, no_improve, patience = -1.0, None, 0, 50\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "for ep in range(500):\n",
    "    model.train()\n",
    "    for bx, by in train_loader:\n",
    "        bx, by = bx.to(device), by.to(device)\n",
    "        logits = model(bx); loss = criterion(logits, by)\n",
    "        optim.zero_grad(); loss.backward(); optim.step()\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        pv, yv = [], []\n",
    "        for bx, by in val_loader:\n",
    "            bx = bx.to(device); logit = model(bx)\n",
    "            pv.append(logit.argmax(1).cpu().numpy()); yv.append(by.numpy())\n",
    "    f1 = f1_score(np.concatenate(yv), np.concatenate(pv), average='macro')\n",
    "    if f1 > best_f1:\n",
    "        best_f1, best_state, no_improve = f1, {k:v.detach().cpu().clone() for k,v in model.state_dict().items()}, 0\n",
    "    else:\n",
    "        no_improve += 1\n",
    "        if no_improve >= patience: break\n",
    "\n",
    "model.load_state_dict({k:v for k,v in best_state.items()})\n",
    "\n",
    "full_ds = TensorDataset(torch.tensor(X_all_std, dtype=torch.float32),\n",
    "                        torch.tensor(y_all0, dtype=torch.long))\n",
    "full_loader = DataLoader(full_ds, batch_size=32, shuffle=True)\n",
    "for ep in range(10):\n",
    "    model.train()\n",
    "    for bx, by in full_loader:\n",
    "        bx, by = bx.to(device), by.to(device)\n",
    "        loss = criterion(model(bx), by)\n",
    "        optim.zero_grad(); loss.backward(); optim.step()\n",
    "\n",
    "pca = PCA(n_components=5, random_state=SEED).fit(X_all_std)\n",
    "T = pca.transform(X_all_std)\n",
    "Pinv = np.linalg.pinv(T.T @ T)\n",
    "h_train = np.einsum('ij,jk,ik->i', T, Pinv, T)\n",
    "h_star  = np.quantile(h_train, 0.95)\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    logits_new = model(torch.tensor(X_new_std, dtype=torch.float32).to(device)).cpu().numpy()\n",
    "prob = torch.softmax(torch.tensor(logits_new), dim=1).numpy()\n",
    "pred = prob.argmax(axis=1) + y_all.min()  \n",
    "T_new = pca.transform(X_new_std)\n",
    "h_new = np.einsum('ij,jk,ik->i', T_new, Pinv, T_new)\n",
    "ad_in = (h_new <= h_star)\n",
    "\n",
    "prob_cols = [f\"Prob_Level_{i}\" for i in range(1, prob.shape[1]+1)]\n",
    "out = pd.concat([\n",
    "    df_new.reset_index(drop=True),\n",
    "    pd.DataFrame({\"Pred_Level\": pred, \"AD_in\": ad_in}),\n",
    "    pd.DataFrame(prob, columns=prob_cols)\n",
    "], axis=1)\n",
    "\n",
    "out[\"max_prob\"] = out[prob_cols].max(axis=1)\n",
    "out.to_excel(os.path.join(OUT_DIR, \"predictions_Xonly.xlsx\"), index=False)\n",
    "print(\"[OK] 保存：predictions_Xonly.xlsx\")\n",
    "\n",
    "top_col = prob_cols[-1]\n",
    "risk_low  = out[out[\"Pred_Level\"]==1].sort_values(top_col, ascending=False)\n",
    "risk_mid  = out[out[\"Pred_Level\"]==2].sort_values(top_col, ascending=False)\n",
    "risk_high = out[out[\"Pred_Level\"]==3].sort_values(top_col, ascending=False)\n",
    "\n",
    "risk_low.to_excel (os.path.join(OUT_DIR, \"risk_low_L1_all.xlsx\"),  index=False)\n",
    "risk_mid.to_excel (os.path.join(OUT_DIR, \"risk_mid_L2_all.xlsx\"),  index=False)\n",
    "risk_high.to_excel(os.path.join(OUT_DIR, \"risk_high_L3_all.xlsx\"), index=False)\n",
    "print(\"[OK] 保存：L1/L2/L3 风险清单（全体）\")\n",
    "\n",
    "core = out[(out[\"AD_in\"]) & (out[\"max_prob\"]>=0.80)].copy()\n",
    "core[core[\"Pred_Level\"]==1].sort_values(top_col, ascending=False)\\\n",
    "    .to_excel(os.path.join(OUT_DIR,\"risk_low_L1_core.xlsx\"), index=False)\n",
    "core[core[\"Pred_Level\"]==2].sort_values(top_col, ascending=False)\\\n",
    "    .to_excel(os.path.join(OUT_DIR,\"risk_mid_L2_core.xlsx\"), index=False)\n",
    "core[core[\"Pred_Level\"]==3].sort_values(top_col, ascending=False)\\\n",
    "    .to_excel(os.path.join(OUT_DIR,\"risk_high_L3_core.xlsx\"), index=False)\n",
    "print(\"[OK] 保存：L1/L2/L3 风险清单（核心：AD_in & max_prob≥0.80）\")\n",
    "\n",
    "SUMMARY_TXT  = os.path.join(OUT_DIR, \"AD_summary.txt\")\n",
    "SUMMARY_XLSX = os.path.join(OUT_DIR, \"AD_breakdown.xlsx\")\n",
    "\n",
    "out[\"ICP_flag\"] = np.select(\n",
    "    [out[\"max_prob\"]>=0.80, out[\"max_prob\"]>=0.60],\n",
    "    [\"HighConf\",\"Borderline\"], default=\"Empty\"\n",
    ")\n",
    "n_total          = len(out)\n",
    "cov_leverage     = float(out[\"AD_in\"].mean())\n",
    "cov_highconf_all = float((out[\"max_prob\"]>=0.80).mean())\n",
    "cov_highconf_in  = float(((out[\"AD_in\"]) & (out[\"max_prob\"]>=0.80)).mean())\n",
    "\n",
    "print(\"\\n=== AD / 置信度摘要 ===\")\n",
    "print(\"样本数：\", n_total)\n",
    "print(\"Leverage h*：\", float(h_star))\n",
    "print(\"Leverage 覆盖率：\", round(cov_leverage,3))\n",
    "print(\"HighConf(≥0.80) 全体占比：\", round(cov_highconf_all,3))\n",
    "print(\"HighConf(≥0.80) AD_in内占比：\", round(cov_highconf_in,3))\n",
    "print(\"ICP_flag 分布：\\n\", out[\"ICP_flag\"].value_counts(normalize=True).round(3))\n",
    "\n",
    "with open(SUMMARY_TXT,\"w\",encoding=\"utf-8\") as f:\n",
    "    f.write(f\"n_total={n_total}\\n\")\n",
    "    f.write(f\"h*={float(h_star):.6f}\\n\")\n",
    "    f.write(f\"cov_leverage={cov_leverage:.3f}\\n\")\n",
    "    f.write(f\"highconf_all={cov_highconf_all:.3f}\\n\")\n",
    "    f.write(f\"highconf_in_AD={cov_highconf_in:.3f}\\n\")\n",
    "    f.write(\"ICP_flag:\\n\")\n",
    "    for k,v in out[\"ICP_flag\"].value_counts(normalize=True).items():\n",
    "        f.write(f\"  {k}: {v:.3f}\\n\")\n",
    "\n",
    "overall_df = pd.DataFrame({\n",
    "    \"metric\":[\"n_total\",\"h_star\",\"cov_leverage\",\"highconf_all\",\"highconf_in_AD\"],\n",
    "    \"value\":[n_total, float(h_star), cov_leverage, cov_highconf_all, cov_highconf_in]\n",
    "})\n",
    "by_icp = out[\"ICP_flag\"].value_counts().to_frame(\"count\")\n",
    "by_icp[\"proportion\"] = by_icp[\"count\"]/n_total\n",
    "\n",
    "with pd.ExcelWriter(SUMMARY_XLSX) as w:\n",
    "    overall_df.to_excel(w, sheet_name=\"overall\", index=False)\n",
    "    by_icp.to_excel(w, sheet_name=\"by_ICP_flag\")\n",
    "    out[[\"AD_in\",\"ICP_flag\",\"max_prob\",top_col]+prob_cols].head(20)\\\n",
    "        .to_excel(w, sheet_name=\"preview_top20\", index=False)\n",
    "\n",
    "print(\"[OK] 保存：AD_summary.txt / AD_breakdown.xlsx\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
